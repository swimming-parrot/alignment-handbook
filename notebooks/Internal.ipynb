{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/envs/lang/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/ubuntu/miniconda3/envs/lang/lib/python3.11/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/home/ubuntu/miniconda3/envs/lang/lib/python3.11/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/home/ubuntu/miniconda3/envs/lang/lib/python3.11/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoConfig, GenerationConfig\n",
    "from src.alignment.model_utils import get_tokenizer\n",
    "from src.alignment.configs import ModelArguments, DataArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config = AutoConfig.from_pretrained('/home/ubuntu/hieu.nn/alignment-handbook/outputs/models/pythia-410m-sft-full/final-model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_args = ModelArguments(\n",
    "    model_name_or_path='/home/ubuntu/hieu.nn/alignment-handbook/outputs/models/pythia-410m-sft-full/final-model',\n",
    "    # model_name_or_path='EleutherAI/pythia-410m',\n",
    "    torch_dtype=\"bfloat16\",\n",
    "    # torch_dtype=\"float16\",\n",
    ")\n",
    "\n",
    "torch_dtype = (\n",
    "    model_args.torch_dtype if model_args.torch_dtype in [\"auto\", None] else getattr(torch, model_args.torch_dtype)\n",
    ")\n",
    "\n",
    "model_kwargs = dict(\n",
    "    revision=model_args.base_model_revision,\n",
    "    trust_remote_code=model_args.trust_remote_code,\n",
    "    use_flash_attention_2=model_args.use_flash_attention_2,\n",
    "    torch_dtype=torch_dtype,\n",
    "    # use_cache=False if training_args.gradient_checkpointing else True,\n",
    "    # device_map=get_kbit_device_map() if quantization_config is not None else None,\n",
    "    # quantization_config=quantization_config,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_args.model_name_or_path,\n",
    "            **model_kwargs\n",
    ")\n",
    "tokenizer = get_tokenizer(model_args, DataArguments())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "genconf = GenerationConfig.from_pretrained(model_args.model_name_or_path)\n",
    "genconf.max_length=200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(\"cuda:5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = torch.tensor(tokenizer.encode(\"Alice was so tired when she got back home so she went\")).unsqueeze_(0).to(\"cuda:5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/envs/lang/lib/python3.11/site-packages/transformers/generation/utils.py:1518: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
      "  warnings.warn(\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    # out = model.generate(sent, genconf)\n",
    "    out = model.generate(sent, max_length=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alice was so tired when she got back home so she went to bed early. She was tired of the constant noise and the constant worry. She didn't want to be a burden to her family.\n",
      "\n",
      "As she lay in bed, she thought about the day she had spent with her parents. She had been so happy and so carefree. She had been so happy to be with her parents. She had been so happy to be with her parents.\n",
      "\n",
      "But then, she had been so worried. She had been so worried about her parents. She had been so worried about her parents.\n",
      "\n",
      "She had been so worried about her parents.\n",
      "\n",
      "She had been so worried about her parents.\n",
      "\n",
      "She had been so worried about her parents.\n",
      "\n",
      "She had been so worried about her parents.\n",
      "\n",
      "She had been so worried about her parents.\n",
      "\n",
      "She had been so worried about her parents.\n",
      "\n",
      "She had been so worried about her parents.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(out.tolist()[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lang",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
